scrapy 框架
    -集成多功能且通用项目的模板
    -了解封装功能的用法
    -高性能的数据存储，异步数据下载，高性能的数据解析，分布式
    -安装 pip install scrapy

创建一个工程：scrapy starrproject xxxPro
    Terminal 中 cd到spider>scray 执行命令scrapy starrproject xxxPro（工程名）
-cd 到 xxxPro
在spider子目录中创建一个爬虫文件：
    -scrapy scrapy spiderName www.xxx.com
-执行工程：
     scrapy crawl spiderName
 -数据解析

 -持久化存储
    -终端指令：
        -要求：只可以将parse方法的返回值存储到本地的文本文件中
        -存储类型：'json', 'jsonlines', 'jl', 'csv', 'xml', 'marshal', 'pickle'
        -指令：scrapy crawl qiubai -o ./fileName.csv     特点：简介快捷，局限性强
    - 管道 ：
        -1.数据解析
        -2.在item类中定义相关属性，将数据封装储存到item类型对象中
        -3.将item对象提交给管道进行持久化存储的操作
        -4.在管道类的 process_item 中要将其接收到的item数据进行持久化存储操作
        -5.配置文件中开启管道
    -将下载文件一份到文件，一份到数据库

    -基于spider全站数据的爬取
        -将网站中某板块下的全部页码对应的页面数据进行爬取和解析
        手动请求发送; yield scrapy.Request(url=new_url,callback=self.parse)

    五大核心组件：
        引擎(Scrapy)
            用来处理整个系统的数据流处理, 触发事务(框架核心)
        调度器(Scheduler)
            用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址
        下载器(Downloader)
            用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)
        爬虫(Spiders)
            爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面
        项目管道(Pipeline)
            负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。

    请求传参：在某些情况下，我们爬取的数据不在同一个页面中，例如，我们爬取一个电影网站，电影的名称，评分在一级页面，而要爬取的其他电影详情在其二级子页面中。这时我们就需要用到请求传参。
            请求传参的使用场景    当我们使用爬虫爬取的数据没有存在于同一张页面的时候，则必须使用请求传参

     提升爬取效率：
     增加并发：
         默认scrapy开启的并发线程为32个，可以适当进行增加。在settings配置文件中修改CONCURRENT_REQUESTS = 100值为100,并发设置成了为100。
     降低日志级别：
         在运行scrapy时，会有大量日志信息的输出，为了减少CPU的使用率。可以设置log输出信息为INFO或者ERROR即可。在配置文件中编写：LOG_LEVEL = ‘INFO’
     禁止cookie：
          如果不是真的需要cookie，则在scrapy爬取数据时可以禁止

          cookie从而减少CPU的使用率，提升爬取效率。在配置文件中编写：COOKIES_ENABLED = False
     禁止重试：
         对失败的HTTP进行重新请求（重试）会减慢爬取速度，因此可以禁止重试。在配置文件中编写：RETRY_ENABLED = False
     减少下载超时：
         如果对一个非常慢的链接进行爬取，减少下载超时可以能让卡住的链接快速被放弃，从而提升效率。在配置文件中进行编写：DOWNLOAD_TIMEOUT = 10 超时时间为10s

       scrapy  图片爬取 -- Imagepiprlines
         在配置文件中进行如下配置：
            IMAGES_STORE = ‘./imgs’：表示最终图片存储的目录
         其实在scrapy中已经为我们封装好了一个专门基于图片请求和持久化存储的管道类ImagesPipeline，那也就是说如果想要基于scrapy实现图片数据的爬取，则可以直接使用该管道类即可。
           from scrapy.pipelines.images import ImagesPipeline

           中间件：引擎和下载器之间
                -可以拦截所有的请求和响应
                    可以进行UA伪装和代理IP更换
                    篡改响应数据，响应对象
    分布式爬虫;scrapy startproject fbsPro
            scrapy genspider -t crawl fbs www.xxx.com
            from scrapy_redis.spiders import RedisCrawlSpider
            注释，添加一个新属性redis_key = 'sun'，是可以被共享的调度器名称
            修改父类RedisCrawlSpider
            修改配置：指定可使用共享的管道和调度器：......
            redis相关配置： linux / mac .conf
            执行 scrapy runspider xx.py
            lpush xxx www.xx.com....

       路飞学城参考文件：https://book.apeland.cn/details/150/